{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T16:15:04.071463Z",
     "iopub.status.busy": "2025-06-27T16:15:04.071183Z",
     "iopub.status.idle": "2025-06-27T16:15:17.405225Z",
     "shell.execute_reply": "2025-06-27T16:15:17.404664Z",
     "shell.execute_reply.started": "2025-06-27T16:15:04.071441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 16:15:05.412744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751040905.574029      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751040905.621207      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1 MODEL (IN PYTHON FOR NOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:31:57.339318Z",
     "iopub.status.busy": "2025-06-27T17:31:57.339027Z",
     "iopub.status.idle": "2025-06-27T17:31:57.352130Z",
     "shell.execute_reply": "2025-06-27T17:31:57.351308Z",
     "shell.execute_reply.started": "2025-06-27T17:31:57.339297Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector: [ 79.   320.     9.   278.    74.     5.    34.32  13.   154.    23.  ]\n",
      "Z-scores: [0.        0.3030303 0.        0.        0.        0.        0.\n",
      " 0.        0.        0.       ]\n",
      "Anomaly Score: 0.03\n",
      "Flags: ['Login from unusual distant location', 'Abnormal speed (>80km/h): 4193.5 km/h']\n",
      "Decision: ESCALATE TO T2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "D = np.array([79, 330, 9, 278, 74, 5, 34.32, 13, 154, 23])  # Reference fingerprint\n",
    "age = 65\n",
    "idle_threshold = 3\n",
    "idle_count = 0\n",
    "\n",
    "last_login_location = (28.7041, 77.1025)  # Delhi\n",
    "current_location = (28.5355, 77.3910)     # Noida (30 km door hai)\n",
    "previous_location = (28.5355, 77.3910)\n",
    "latest_location = (28.6038, 77.0417)\n",
    "\n",
    "v = np.array([79, 320, 9, 278, 74, 5, 34.32, 13, 154, 23]) \n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2\n",
    "    return 2 * atan2(sqrt(a), sqrt(1 - a)) * R\n",
    "\n",
    "def estimate_std(D):\n",
    "    return np.maximum(1e-2, np.abs(D * 0.10))  # 10% tolerance\n",
    "\n",
    "def compute_anomaly_score(v, D_mean, D_std, age):\n",
    "    if np.count_nonzero(v) < 4:\n",
    "        return None, None  # Skip interval\n",
    "    z = np.abs((v - D_mean) / (D_std + 1e-8))\n",
    "    age_boost = 1.15 if age >= 60 else 1\n",
    "    return np.mean(z) * age_boost, z\n",
    "\n",
    "def rule_based_checks(v, last_loc, curr_loc, prev_loc, new_loc):\n",
    "    flags = []\n",
    "\n",
    "    # 1. Unusual login location\n",
    "    if haversine(*last_loc, *curr_loc) > 10:\n",
    "        flags.append(\"Login from unusual distant location\")\n",
    "\n",
    "    # 2. Travel speed\n",
    "    session_km = haversine(*prev_loc, *new_loc)\n",
    "    speed = session_km / (30 / 3600)\n",
    "    if speed > 80:\n",
    "        flags.append(f\"Abnormal speed (>80km/h): {speed:.1f} km/h\")\n",
    "\n",
    "    # 3. Behavioral thresholds\n",
    "    if v[0] != 0 and (v[0] < 50 or v[0] > 98): flags.append(\"Unusual accuracy\")\n",
    "    if v[1] != 0 and v[1] > 600: flags.append(\"Flight time too high\")\n",
    "    if v[5] != 0 and v[5] > 10: flags.append(\"Error rate too high\")\n",
    "\n",
    "    return flags, speed\n",
    "\n",
    "# ------------------- DECISION -------------------\n",
    "D_std = estimate_std(D)\n",
    "anomaly_score, z_scores = compute_anomaly_score(v, D, D_std, age)\n",
    "rule_flags, speed = rule_based_checks(v, last_login_location, current_location, previous_location, latest_location)\n",
    "\n",
    "THRESHOLD_PASS = 1.5\n",
    "THRESHOLD_ESCALATE_T2 = 2.5\n",
    "\n",
    "\n",
    "\n",
    "if anomaly_score is None:\n",
    "    idle_count += 1\n",
    "    if idle_count >= idle_threshold and speed > 80:\n",
    "        decision = \"ESCALATE TO T2 (Idle + Abnormal Travel)\"\n",
    "    else:\n",
    "        decision = \"SKIP (Idle)\"\n",
    "else:\n",
    "    idle_count = 0\n",
    "    if anomaly_score < THRESHOLD_PASS and not rule_flags:\n",
    "        decision = \"PASS\"\n",
    "    elif anomaly_score < THRESHOLD_ESCALATE_T2 or any(f in rule_flags):\n",
    "        decision = \"ESCALATE TO T2\"\n",
    "    else:\n",
    "        decision = \"ESCALATE TO T3\"\n",
    "\n",
    "# main\n",
    "print(f\"Feature vector: {v}\")\n",
    "print(f\"Z-scores: {z_scores if z_scores is not None else 'N/A'}\")\n",
    "print(f\"Anomaly Score: {anomaly_score:.2f}\" if anomaly_score else \"Anomaly Score: N/A\")\n",
    "print(\"Flags:\", rule_flags)\n",
    "print(\"Decision:\", decision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2 MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T17:14:37.676904Z",
     "iopub.status.busy": "2025-06-24T17:14:37.676604Z",
     "iopub.status.idle": "2025-06-24T17:15:02.055987Z",
     "shell.execute_reply": "2025-06-24T17:15:02.054935Z",
     "shell.execute_reply.started": "2025-06-24T17:14:37.676877Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/tmp/ipykernel_35/1739784229.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  anchors = torch.tensor([x[0] for x in batch], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 0.3190\n",
      "Epoch 2/50 | Loss: 0.1512\n",
      "Epoch 3/50 | Loss: 0.1062\n",
      "Epoch 4/50 | Loss: 0.0978\n",
      "Epoch 5/50 | Loss: 0.0806\n",
      "Epoch 6/50 | Loss: 0.0845\n",
      "Epoch 7/50 | Loss: 0.0841\n",
      "Epoch 8/50 | Loss: 0.0952\n",
      "Epoch 9/50 | Loss: 0.0770\n",
      "Epoch 10/50 | Loss: 0.0828\n",
      "Epoch 11/50 | Loss: 0.0664\n",
      "Epoch 12/50 | Loss: 0.0647\n",
      "Epoch 13/50 | Loss: 0.0633\n",
      "Epoch 14/50 | Loss: 0.0617\n",
      "Epoch 15/50 | Loss: 0.0666\n",
      "Epoch 16/50 | Loss: 0.0589\n",
      "Epoch 17/50 | Loss: 0.0499\n",
      "Epoch 18/50 | Loss: 0.0453\n",
      "Epoch 19/50 | Loss: 0.0491\n",
      "Epoch 20/50 | Loss: 0.0374\n",
      "Epoch 21/50 | Loss: 0.0486\n",
      "Epoch 22/50 | Loss: 0.0401\n",
      "Epoch 23/50 | Loss: 0.0435\n",
      "Epoch 24/50 | Loss: 0.0495\n",
      "Epoch 25/50 | Loss: 0.0444\n",
      "Epoch 26/50 | Loss: 0.0373\n",
      "Epoch 27/50 | Loss: 0.0293\n",
      "Epoch 28/50 | Loss: 0.0459\n",
      "Epoch 29/50 | Loss: 0.0335\n",
      "Epoch 30/50 | Loss: 0.0275\n",
      "Epoch 31/50 | Loss: 0.0306\n",
      "Epoch 32/50 | Loss: 0.0424\n",
      "Epoch 33/50 | Loss: 0.0332\n",
      "Epoch 34/50 | Loss: 0.0491\n",
      "Epoch 35/50 | Loss: 0.0279\n",
      "Epoch 36/50 | Loss: 0.0365\n",
      "Epoch 37/50 | Loss: 0.0279\n",
      "Epoch 38/50 | Loss: 0.0342\n",
      "Epoch 39/50 | Loss: 0.0213\n",
      "Epoch 40/50 | Loss: 0.0345\n",
      "Epoch 41/50 | Loss: 0.0227\n",
      "Epoch 42/50 | Loss: 0.0392\n",
      "Epoch 43/50 | Loss: 0.0434\n",
      "Epoch 44/50 | Loss: 0.0420\n",
      "Epoch 45/50 | Loss: 0.0285\n",
      "Epoch 46/50 | Loss: 0.0230\n",
      "Epoch 47/50 | Loss: 0.0271\n",
      "Epoch 48/50 | Loss: 0.0234\n",
      "Epoch 49/50 | Loss: 0.0200\n",
      "Epoch 50/50 | Loss: 0.0164\n",
      "Distance between user 0 and 1 sample: 2.8719\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Synthetic data generator\n",
    "def generate_user_profile():\n",
    "    accuracy = np.random.uniform(70, 95)\n",
    "    flight_time = np.random.uniform(250, 400)\n",
    "    hold_time = np.random.uniform(7, 12)\n",
    "    tap_rhythm = np.random.uniform(250, 320)\n",
    "    correct_chars = int(accuracy / 100 * 100)\n",
    "    error_rate = 100 - accuracy\n",
    "    total_time = np.random.uniform(30, 50)\n",
    "    total_words = int(correct_chars / 5)\n",
    "    cpm = correct_chars / (total_time / 60)\n",
    "    wpm = total_words / (total_time / 60)\n",
    "    return np.array([accuracy, flight_time, hold_time, tap_rhythm,\n",
    "                     correct_chars, error_rate, total_time, total_words, cpm, wpm])\n",
    "\n",
    "def generate_sample(user_base):\n",
    "    jitter = np.array([\n",
    "        np.random.normal(0, 2),\n",
    "        np.random.normal(0, 10),\n",
    "        np.random.normal(0, 0.5),\n",
    "        np.random.normal(0, 10),\n",
    "        np.random.normal(0, 2),\n",
    "        np.random.normal(0, 1),\n",
    "        np.random.normal(0, 1),\n",
    "        np.random.normal(0, 1),\n",
    "        np.random.normal(0, 5),\n",
    "        np.random.normal(0, 1)\n",
    "    ])\n",
    "    return user_base + jitter\n",
    "\n",
    "num_users = 20\n",
    "samples_per_user = 15\n",
    "input_dim = 10\n",
    "\n",
    "users_data = {}\n",
    "for u in range(num_users):\n",
    "    base = generate_user_profile()\n",
    "    users_data[u] = np.array([generate_sample(base) for _ in range(samples_per_user)])\n",
    "\n",
    "# Normalize\n",
    "all_samples = np.vstack([users_data[u] for u in users_data])\n",
    "scaler = StandardScaler().fit(all_samples)\n",
    "for u in users_data:\n",
    "    users_data[u] = scaler.transform(users_data[u])\n",
    "\n",
    "def create_triplets(users_data):\n",
    "    triplets = []\n",
    "    users = list(users_data.keys())\n",
    "    for u in users:\n",
    "        positives = users_data[u]\n",
    "        for i in range(len(positives)):\n",
    "            anchor = positives[i]\n",
    "            pos = positives[np.random.choice([j for j in range(len(positives)) if j != i])]\n",
    "            neg_user = np.random.choice([x for x in users if x != u])\n",
    "            neg_pool = users_data[neg_user]\n",
    "            distances = np.linalg.norm(neg_pool - anchor, axis=1)\n",
    "            hard_neg = neg_pool[np.argmin(distances)]\n",
    "            triplets.append((anchor, pos, hard_neg))\n",
    "    return triplets\n",
    "\n",
    "triplets = create_triplets(users_data)\n",
    "\n",
    "# model architecture\n",
    "class ComplexSiameseNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(input_dim, 256)),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(256, 128)),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(128, 128)),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(128, 64)),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(64, 32)),\n",
    "            nn.LayerNorm(32),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.block6 = nn.Linear(32, 8)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = x + self.block3(x)  # residual\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, a, p, n):\n",
    "        return self.forward_once(a), self.forward_once(p), self.forward_once(n)\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.5):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, a, p, n):\n",
    "        pos_dist = F.pairwise_distance(a, p)\n",
    "        neg_dist = F.pairwise_distance(a, n)\n",
    "        return F.relu(pos_dist - neg_dist + self.margin).mean()\n",
    "\n",
    "#mdoel training\n",
    "model = ComplexSiameseNet(input_dim)\n",
    "criterion = TripletLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(triplets)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for i in range(0, len(triplets), batch_size):\n",
    "        batch = triplets[i:i+batch_size]\n",
    "        anchors = torch.tensor([x[0] for x in batch], dtype=torch.float32)\n",
    "        pos = torch.tensor([x[1] for x in batch], dtype=torch.float32)\n",
    "        neg = torch.tensor([x[2] for x in batch], dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        a_out, p_out, n_out = model(anchors, pos, neg)\n",
    "        loss = criterion(a_out, p_out, n_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(batch)\n",
    "    avg_loss = total_loss / len(triplets)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "def compute_embedding(model, v):\n",
    "    with torch.no_grad():\n",
    "        v = torch.tensor(v, dtype=torch.float32).unsqueeze(0)\n",
    "        emb = model.forward_once(v)\n",
    "        return emb.squeeze().numpy()\n",
    "\n",
    "# Example: compare user 0's sample with user 1's sample\n",
    "emb1 = compute_embedding(model, users_data[0][0])\n",
    "emb2 = compute_embedding(model, users_data[1][0])\n",
    "distance = np.linalg.norm(emb1 - emb2)\n",
    "print(f\"Distance between user 0 and 1 sample: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T18:57:21.643601Z",
     "iopub.status.busy": "2025-06-22T18:57:21.642885Z",
     "iopub.status.idle": "2025-06-22T18:57:21.653674Z",
     "shell.execute_reply": "2025-06-22T18:57:21.652892Z",
     "shell.execute_reply.started": "2025-06-22T18:57:21.643569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between user 0 and 1 sample: 3.4323\n"
     ]
    }
   ],
   "source": [
    "def compute_embedding(model, v):\n",
    "    with torch.no_grad():\n",
    "        v = torch.tensor(v, dtype=torch.float32).unsqueeze(0)\n",
    "        emb = model.forward_once(v)\n",
    "        return emb.squeeze().numpy()\n",
    "\n",
    "# Example: compare user 0's sample with user 1's sample\n",
    "emb1 = compute_embedding(model, users_data[0][0])\n",
    "emb2 = compute_embedding(model, users_data[1][0])\n",
    "distance = np.linalg.norm(emb1 - emb2)\n",
    "print(f\"Distance between user 0 and 1 sample: {distance:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
