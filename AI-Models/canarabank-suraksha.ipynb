{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom sklearn.preprocessing import StandardScaler\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T18:06:42.233265Z","iopub.execute_input":"2025-06-22T18:06:42.234230Z","iopub.status.idle":"2025-06-22T18:06:42.237691Z","shell.execute_reply.started":"2025-06-22T18:06:42.234207Z","shell.execute_reply":"2025-06-22T18:06:42.237027Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#distance beteen 2 GPS coordinates\n\ndef calculate_distance(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculate the great-circle distance between two GPS points using the Haversine formula.\n\n    Parameters:\n    lat1, lon1 -- latitude and longitude of first point (in decimal degrees)\n    lat2, lon2 -- latitude and longitude of second point (in decimal degrees)\n\n    Returns:\n    Distance in kilometers\n    \"\"\"\n    # Earth radius in kilometers\n    R = 6371.0\n\n    # Convert degrees to radians\n    phi1 = math.radians(lat1)\n    phi2 = math.radians(lat2)\n    delta_phi = math.radians(lat2 - lat1)\n    delta_lambda = math.radians(lon2 - lon1)\n\n    # Haversine formula\n    a = math.sin(delta_phi / 2) ** 2 + \\\n        math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2) ** 2\n\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\n    distance = R * c\n\n    return distance\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example feature names: keystroke timing (ms), mouse speed (px/s), click rate (clicks/min)\nfeature_names = ['keystroke_mean', 'keystroke_std', 'mouse_speed', 'click_rate']\n\n# Assume at registration you captured these for the user:\nD_mean = np.array([120, 30, 500, 20])   # Example mean template\nD_std = np.array([10, 5, 100, 5])       # Example std dev template\n\n# Live session feature vector (captured every 30s)\nv = np.array([150, 40, 600, 25])  # Example observed features\n\n# Compute z-scores\nz = np.abs((v - D_mean) / D_std)\n\n# Anomaly score = average z-score (or could be weighted sum)\nanomaly_score = np.mean(z)\n\nprint(f\"Z-scores: {z}\")\nprint(f\"Anomaly score: {anomaly_score:.2f}\")\n\n# Rule-based checks\nrule_flags = []\n\n# Example: abnormal mouse speed\nif v[2] > 1000:\n    rule_flags.append(\"Abnormally high mouse speed\")\n\n# Example: keystroke mean outside realistic range\nif v[0] > 300 or v[0] < 50:\n    rule_flags.append(\"Keystroke timing abnormal\")\n\nloc_distance = calculate_distance() # values to be added\nif loc_distance>10 and not travel_flag{\n    rule_flags.append(\"Suspicion in location\")\n}\n\n# Decision thresholds\nTHRESHOLD_PASS = 1.5\nTHRESHOLD_ESCALATE_T2 = 2.5\n\n# Combine score + rules\nif anomaly_score < THRESHOLD_PASS and not rule_flags:\n    decision = \"PASS\"\nelif anomaly_score < THRESHOLD_ESCALATE_T2 and not rule_flags:\n    decision = \"ESCALATE TO T2\"\nelse:\n    decision = \"ESCALATE TO T3\"\n\nprint(f\"Decision: {decision}\")\nif rule_flags:\n    print(\"Rule flags triggered:\")\n    for flag in rule_flags:\n        print(f\" - {flag}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom sklearn.preprocessing import StandardScaler\n\n# === 1️⃣ Synthetic data generator ===\ndef generate_user_profile():\n    accuracy = np.random.uniform(70, 95)\n    flight_time = np.random.uniform(250, 400)\n    hold_time = np.random.uniform(7, 12)\n    tap_rhythm = np.random.uniform(250, 320)\n    correct_chars = int(accuracy / 100 * 100)\n    error_rate = 100 - accuracy\n    total_time = np.random.uniform(30, 50)\n    total_words = int(correct_chars / 5)\n    cpm = correct_chars / (total_time / 60)\n    wpm = total_words / (total_time / 60)\n    return np.array([accuracy, flight_time, hold_time, tap_rhythm,\n                     correct_chars, error_rate, total_time, total_words, cpm, wpm])\n\ndef generate_sample(user_base):\n    jitter = np.array([\n        np.random.normal(0, 2),\n        np.random.normal(0, 10),\n        np.random.normal(0, 0.5),\n        np.random.normal(0, 10),\n        np.random.normal(0, 2),\n        np.random.normal(0, 1),\n        np.random.normal(0, 1),\n        np.random.normal(0, 1),\n        np.random.normal(0, 5),\n        np.random.normal(0, 1)\n    ])\n    return user_base + jitter\n\nnum_users = 20\nsamples_per_user = 15\ninput_dim = 10\n\nusers_data = {}\nfor u in range(num_users):\n    base = generate_user_profile()\n    users_data[u] = np.array([generate_sample(base) for _ in range(samples_per_user)])\n\n# Normalize\nall_samples = np.vstack([users_data[u] for u in users_data])\nscaler = StandardScaler().fit(all_samples)\nfor u in users_data:\n    users_data[u] = scaler.transform(users_data[u])\n\ndef create_triplets(users_data):\n    triplets = []\n    users = list(users_data.keys())\n    for u in users:\n        positives = users_data[u]\n        for i in range(len(positives)):\n            anchor = positives[i]\n            pos = positives[np.random.choice([j for j in range(len(positives)) if j != i])]\n            neg_user = np.random.choice([x for x in users if x != u])\n            neg_pool = users_data[neg_user]\n            distances = np.linalg.norm(neg_pool - anchor, axis=1)\n            hard_neg = neg_pool[np.argmin(distances)]\n            triplets.append((anchor, pos, hard_neg))\n    return triplets\n\ntriplets = create_triplets(users_data)\n\n# === 2️⃣ Complex model ===\nclass ComplexSiameseNet(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.block1 = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(input_dim, 256)),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        self.block2 = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(256, 128)),\n            nn.LayerNorm(128),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        self.block3 = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(128, 128)),\n            nn.LayerNorm(128),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        self.block4 = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(128, 64)),\n            nn.LayerNorm(64),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        self.block5 = nn.Sequential(\n            nn.utils.weight_norm(nn.Linear(64, 32)),\n            nn.LayerNorm(32),\n            nn.GELU()\n        )\n        self.block6 = nn.Linear(32, 8)\n\n    def forward_once(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = x + self.block3(x)  # residual\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        return x\n\n    def forward(self, a, p, n):\n        return self.forward_once(a), self.forward_once(p), self.forward_once(n)\n\nclass TripletLoss(nn.Module):\n    def __init__(self, margin=0.5):\n        super().__init__()\n        self.margin = margin\n\n    def forward(self, a, p, n):\n        pos_dist = F.pairwise_distance(a, p)\n        neg_dist = F.pairwise_distance(a, n)\n        return F.relu(pos_dist - neg_dist + self.margin).mean()\n\n# === 3️⃣ Train ===\nmodel = ComplexSiameseNet(input_dim)\ncriterion = TripletLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n\nepochs = 50\nbatch_size = 16\nfor epoch in range(epochs):\n    random.shuffle(triplets)\n    total_loss = 0\n    model.train()\n    for i in range(0, len(triplets), batch_size):\n        batch = triplets[i:i+batch_size]\n        anchors = torch.tensor([x[0] for x in batch], dtype=torch.float32)\n        pos = torch.tensor([x[1] for x in batch], dtype=torch.float32)\n        neg = torch.tensor([x[2] for x in batch], dtype=torch.float32)\n\n        optimizer.zero_grad()\n        a_out, p_out, n_out = model(anchors, pos, neg)\n        loss = criterion(a_out, p_out, n_out)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * len(batch)\n    avg_loss = total_loss / len(triplets)\n    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n\ndef compute_embedding(model, v):\n    with torch.no_grad():\n        v = torch.tensor(v, dtype=torch.float32).unsqueeze(0)\n        emb = model.forward_once(v)\n        return emb.squeeze().numpy()\n\n# Example: compare user 0's sample with user 1's sample\nemb1 = compute_embedding(model, users_data[0][0])\nemb2 = compute_embedding(model, users_data[1][0])\ndistance = np.linalg.norm(emb1 - emb2)\nprint(f\"Distance between user 0 and 1 sample: {distance:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T17:14:37.676604Z","iopub.execute_input":"2025-06-24T17:14:37.676904Z","iopub.status.idle":"2025-06-24T17:15:02.055987Z","shell.execute_reply.started":"2025-06-24T17:14:37.676877Z","shell.execute_reply":"2025-06-24T17:15:02.054935Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n  WeightNorm.apply(module, name, dim)\n/tmp/ipykernel_35/1739784229.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  anchors = torch.tensor([x[0] for x in batch], dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50 | Loss: 0.3190\nEpoch 2/50 | Loss: 0.1512\nEpoch 3/50 | Loss: 0.1062\nEpoch 4/50 | Loss: 0.0978\nEpoch 5/50 | Loss: 0.0806\nEpoch 6/50 | Loss: 0.0845\nEpoch 7/50 | Loss: 0.0841\nEpoch 8/50 | Loss: 0.0952\nEpoch 9/50 | Loss: 0.0770\nEpoch 10/50 | Loss: 0.0828\nEpoch 11/50 | Loss: 0.0664\nEpoch 12/50 | Loss: 0.0647\nEpoch 13/50 | Loss: 0.0633\nEpoch 14/50 | Loss: 0.0617\nEpoch 15/50 | Loss: 0.0666\nEpoch 16/50 | Loss: 0.0589\nEpoch 17/50 | Loss: 0.0499\nEpoch 18/50 | Loss: 0.0453\nEpoch 19/50 | Loss: 0.0491\nEpoch 20/50 | Loss: 0.0374\nEpoch 21/50 | Loss: 0.0486\nEpoch 22/50 | Loss: 0.0401\nEpoch 23/50 | Loss: 0.0435\nEpoch 24/50 | Loss: 0.0495\nEpoch 25/50 | Loss: 0.0444\nEpoch 26/50 | Loss: 0.0373\nEpoch 27/50 | Loss: 0.0293\nEpoch 28/50 | Loss: 0.0459\nEpoch 29/50 | Loss: 0.0335\nEpoch 30/50 | Loss: 0.0275\nEpoch 31/50 | Loss: 0.0306\nEpoch 32/50 | Loss: 0.0424\nEpoch 33/50 | Loss: 0.0332\nEpoch 34/50 | Loss: 0.0491\nEpoch 35/50 | Loss: 0.0279\nEpoch 36/50 | Loss: 0.0365\nEpoch 37/50 | Loss: 0.0279\nEpoch 38/50 | Loss: 0.0342\nEpoch 39/50 | Loss: 0.0213\nEpoch 40/50 | Loss: 0.0345\nEpoch 41/50 | Loss: 0.0227\nEpoch 42/50 | Loss: 0.0392\nEpoch 43/50 | Loss: 0.0434\nEpoch 44/50 | Loss: 0.0420\nEpoch 45/50 | Loss: 0.0285\nEpoch 46/50 | Loss: 0.0230\nEpoch 47/50 | Loss: 0.0271\nEpoch 48/50 | Loss: 0.0234\nEpoch 49/50 | Loss: 0.0200\nEpoch 50/50 | Loss: 0.0164\nDistance between user 0 and 1 sample: 2.8719\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def compute_embedding(model, v):\n    with torch.no_grad():\n        v = torch.tensor(v, dtype=torch.float32).unsqueeze(0)\n        emb = model.forward_once(v)\n        return emb.squeeze().numpy()\n\n# Example: compare user 0's sample with user 1's sample\nemb1 = compute_embedding(model, users_data[0][0])\nemb2 = compute_embedding(model, users_data[1][0])\ndistance = np.linalg.norm(emb1 - emb2)\nprint(f\"Distance between user 0 and 1 sample: {distance:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T18:57:21.642885Z","iopub.execute_input":"2025-06-22T18:57:21.643601Z","iopub.status.idle":"2025-06-22T18:57:21.653674Z","shell.execute_reply.started":"2025-06-22T18:57:21.643569Z","shell.execute_reply":"2025-06-22T18:57:21.652892Z"}},"outputs":[{"name":"stdout","text":"Distance between user 0 and 1 sample: 3.4323\n","output_type":"stream"}],"execution_count":12}]}